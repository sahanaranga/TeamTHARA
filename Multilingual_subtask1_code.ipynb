{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqiN-ReuEttz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Task1Dataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    Dataset provided for SemEval 2024 Task 1 Challenge\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path, class_list):\n",
        "\n",
        "        \"\"\"\n",
        "        path: path to the json file containing data samples\n",
        "        class_list: list of classes present for a task\n",
        "        \"\"\"\n",
        "\n",
        "        self.path = path\n",
        "        self.class_list = read_classes(class_list)\n",
        "\n",
        "        with open(self.path, 'r') as file:\n",
        "            self.data = json.load(file)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        sample = self.data[index]\n",
        "\n",
        "        sample_id = sample['id']\n",
        "        text = sample['text']\n",
        "        labels = sample['labels']\n",
        "\n",
        "        labels_id = [self.class_list.index(x) for x in labels]\n",
        "\n",
        "        return sample_id, text, labels_id\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcG6DIVJ_cA2"
      },
      "outputs": [],
      "source": [
        "def read_classes(file_path):\n",
        "\n",
        "    classes = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "\n",
        "        for label in f.readlines():\n",
        "            label = label.strip()\n",
        "\n",
        "            if label:\n",
        "                classes.append(label)\n",
        "\n",
        "    return classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4D5N6t1aJMn"
      },
      "outputs": [],
      "source": [
        "import pdb\n",
        "import json\n",
        "import logging.handlers\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from networkx import DiGraph, relabel_nodes, all_pairs_shortest_path_length\n",
        "from sklearn_hierarchical_classification.constants import ROOT\n",
        "from sklearn_hierarchical_classification.metrics import h_fbeta_score, h_recall_score, h_precision_score, \\\n",
        "    fill_ancestors, multi_labeled\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "\n",
        "KEYS = ['id','labels']\n",
        "logger = logging.getLogger(\"subtask_1_2a_scorer\")\n",
        "ch = logging.StreamHandler(sys.stdout)\n",
        "ch.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "ch.setFormatter(formatter)\n",
        "logger.setLevel(logging.INFO)\n",
        "#logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "G = DiGraph()\n",
        "G.add_edge(ROOT, \"Logos\")\n",
        "G.add_edge(\"Logos\", \"Repetition\")\n",
        "G.add_edge(\"Logos\", \"Obfuscation, Intentional vagueness, Confusion\")\n",
        "G.add_edge(\"Logos\", \"Reasoning\")\n",
        "G.add_edge(\"Logos\", \"Justification\")\n",
        "G.add_edge('Justification', \"Slogans\")\n",
        "G.add_edge('Justification', \"Bandwagon\")\n",
        "G.add_edge('Justification', \"Appeal to authority\")\n",
        "G.add_edge('Justification', \"Flag-waving\")\n",
        "G.add_edge('Justification', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Reasoning', \"Simplification\")\n",
        "G.add_edge('Simplification', \"Causal Oversimplification\")\n",
        "G.add_edge('Simplification', \"Black-and-white Fallacy/Dictatorship\")\n",
        "G.add_edge('Simplification', \"Thought-terminating clichÃ©\")\n",
        "G.add_edge('Reasoning', \"Distraction\")\n",
        "G.add_edge('Distraction', \"Misrepresentation of Someone's Position (Straw Man)\")\n",
        "G.add_edge('Distraction', \"Presenting Irrelevant Data (Red Herring)\")\n",
        "G.add_edge('Distraction', \"Whataboutism\")\n",
        "G.add_edge(ROOT, \"Ethos\")\n",
        "G.add_edge('Ethos', \"Appeal to authority\")\n",
        "G.add_edge('Ethos', \"Glittering generalities (Virtue)\")\n",
        "G.add_edge('Ethos', \"Bandwagon\")\n",
        "G.add_edge('Ethos', \"Ad Hominem\")\n",
        "G.add_edge('Ethos', \"Transfer\")\n",
        "G.add_edge('Ad Hominem', \"Doubt\")\n",
        "G.add_edge('Ad Hominem', \"Name calling/Labeling\")\n",
        "G.add_edge('Ad Hominem', \"Smears\")\n",
        "G.add_edge('Ad Hominem', \"Reductio ad hitlerum\")\n",
        "G.add_edge('Ad Hominem', \"Whataboutism\")\n",
        "G.add_edge(ROOT, \"Pathos\")\n",
        "G.add_edge('Pathos', \"Exaggeration/Minimisation\")\n",
        "G.add_edge('Pathos', \"Loaded Language\")\n",
        "G.add_edge('Pathos', \"Appeal to (Strong) Emotions\")\n",
        "G.add_edge('Pathos', \"Appeal to fear/prejudice\")\n",
        "G.add_edge('Pathos', \"Flag-waving\")\n",
        "G.add_edge('Pathos', \"Transfer\")\n",
        "\n",
        "def get_all_classes_from_graph(graph):\n",
        "    return [\n",
        "        node\n",
        "        for node in graph.nodes\n",
        "        if node != ROOT\n",
        "        ]\n",
        "\n",
        "def _h_fbeta_score(y_true, y_pred, class_hierarchy, beta=1., root=ROOT):\n",
        "    hP = _h_precision_score(y_true, y_pred, class_hierarchy, root=root)\n",
        "    hR = _h_recall_score(y_true, y_pred, class_hierarchy, root=root)\n",
        "\n",
        "    if hP == 0 and hR == 0:\n",
        "       return 0\n",
        "\n",
        "    return (1. + beta ** 2.) * hP * hR / (beta ** 2. * hP + hR)\n",
        "\n",
        "def _fill_ancestors(y, graph, root, copy=True):\n",
        "    y_ = y.copy() if copy else y\n",
        "    paths = all_pairs_shortest_path_length(graph.reverse(copy=False))\n",
        "    for target, distances in paths:\n",
        "        if target == root:\n",
        "            continue\n",
        "        ix_rows = np.where(y[:, target] > 0)[0]\n",
        "        ancestors = list(filter(lambda x: x != ROOT,distances.keys()))\n",
        "        y_[tuple(np.meshgrid(ix_rows, ancestors))] = 1\n",
        "    graph.reverse(copy=False)\n",
        "    return y_\n",
        "\n",
        "def _h_recall_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
        "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
        "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
        "\n",
        "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
        "\n",
        "    true_positives = len(ix[0])\n",
        "    all_positives = np.count_nonzero(y_true_)\n",
        "\n",
        "    if all_positives == 0:\n",
        "        return 0\n",
        "\n",
        "    return true_positives / all_positives\n",
        "\n",
        "def _h_precision_score(y_true, y_pred, class_hierarchy, root=ROOT):\n",
        "    y_true_ = _fill_ancestors(y_true, graph=class_hierarchy, root=root)\n",
        "    y_pred_ = _fill_ancestors(y_pred, graph=class_hierarchy, root=root)\n",
        "\n",
        "    ix = np.where((y_true_ != 0) & (y_pred_ != 0))\n",
        "\n",
        "    true_positives = len(ix[0])\n",
        "    all_results = np.count_nonzero(y_pred_)\n",
        "\n",
        "    if all_results == 0:\n",
        "      return 0\n",
        "\n",
        "    return true_positives / all_results\n",
        "\n",
        "def read_classes(file_path):\n",
        "  CLASSES = []\n",
        "  with open(file_path) as f:\n",
        "    for label in f.readlines():\n",
        "      label = label.strip()\n",
        "      if label:\n",
        "        CLASSES.append(label)\n",
        "  return CLASSES\n",
        "\n",
        "def check_format(file_path):\n",
        "  _classes = get_all_classes_from_graph(G)\n",
        "  if not os.path.exists(file_path):\n",
        "    logging.error(\"File doesnt exists: {}\".format(file_path))\n",
        "    return False\n",
        "  submmission = ''\n",
        "  try:\n",
        "    with open(file_path, encoding='utf-8') as p:\n",
        "      submission = json.load(p)\n",
        "  except:\n",
        "    logging.error(\"File is not a valid json file: {}\".format(file_path))\n",
        "    return False\n",
        "  for i, obj in enumerate(submission):\n",
        "    for key in KEYS:\n",
        "      if key not in obj:\n",
        "        logging.error(\"Missing entry in {}:{}\".format(file_path, i))\n",
        "        return False\n",
        "  for label in list(obj['labels']):\n",
        "       if label not in _classes:\n",
        "         print(label)\n",
        "         logging.error(\"Unknown Label in {}:{}\".format(file_path, i))\n",
        "         return False\n",
        "  return True\n",
        "\n",
        "def _read_gold_and_pred(pred_fpath, gold_fpath):\n",
        "  \"\"\"\n",
        "  Read gold and predicted data.\n",
        "  :param pred_fpath: a json file with predictions,\n",
        "  :param gold_fpath: the original annotated gold file.\n",
        "  :return: {id:pred_labels} dict; {id:gold_labels} dict\n",
        "  \"\"\"\n",
        "\n",
        "  gold_labels = {}\n",
        "  with open(gold_fpath, encoding='utf-8') as gold_f:\n",
        "    gold = json.load(gold_f)\n",
        "    for obj in gold:\n",
        "      gold_labels[obj['id']] = obj['labels']\n",
        "\n",
        "  pred_labels = {}\n",
        "  with open(pred_fpath, encoding='utf-8') as pred_f:\n",
        "    pred = json.load(pred_f)\n",
        "    for obj in pred:\n",
        "      pred_labels[obj['id']] = obj['labels']\n",
        "\n",
        "  if set(gold_labels.keys()) != set(pred_labels.keys()):\n",
        "      logger.error('There are either missing or added examples to the prediction file. Make sure you only have the gold examples in the prediction file.')\n",
        "      raise ValueError('There are either missing or added examples to the prediction file. Make sure you only have the gold examples in the prediction file.')\n",
        "\n",
        "  return pred_labels, gold_labels\n",
        "\n",
        "# def evaluate_h(pred_fpath, gold_fpath):\n",
        "def evaluate_h(pred_file, gold_file):\n",
        "    if validate_files(pred_file, gold_file):\n",
        "      pred_labels, gold_labels = _read_gold_and_pred(pred_file, gold_file)\n",
        "\n",
        "      gold = []\n",
        "      pred = []\n",
        "      for id in gold_labels:\n",
        "          gold.append(gold_labels[id])\n",
        "          pred.append(pred_labels[id])\n",
        "      with multi_labeled(gold, pred, G) as (gold_, pred_, graph_):\n",
        "          return  _h_precision_score(gold_, pred_,graph_), _h_recall_score(gold_, pred_,graph_), _h_fbeta_score(gold_, pred_,graph_)\n",
        "\n",
        "def validate_files(pred_files, gold_files):\n",
        "  if not check_format(pred_files):\n",
        "    logger.error('Bad format for pred file {}. Cannot score.'.format(pred_files))\n",
        "    return False\n",
        "  return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUwOXasXL_bJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#import task1_dataset\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "class Collate:\n",
        "\n",
        "\n",
        "    def __init__(self, class_list):\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "        self.class_list = read_classes(class_list)\n",
        "\n",
        "\n",
        "    def __call__(self, data):\n",
        "\n",
        "        ids, texts, labels = zip(*data)\n",
        "\n",
        "        tokenized_text = []\n",
        "        attention_masks = []\n",
        "\n",
        "        for sent in texts:\n",
        "            preprocessed_sent = sent.replace('\\\\n', ' ').strip()\n",
        "\n",
        "            encoded_sent = self.tokenizer.encode_plus(\n",
        "                text=preprocessed_sent,\n",
        "                add_special_tokens=True,\n",
        "                # max_length=512, #self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_attention_mask=True\n",
        "            )\n",
        "\n",
        "            tokenized_text.append(encoded_sent.get('input_ids'))\n",
        "            attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "\n",
        "        text_input = torch.tensor(tokenized_text)\n",
        "        attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "        texts_len = len(texts)\n",
        "        labels_output = torch.zeros(texts_len, len(self.class_list))\n",
        "\n",
        "        # creating one-hot vector of labels for multi-label classification\n",
        "        for lo, c in zip(labels_output, labels):\n",
        "            lo[c] = 1\n",
        "\n",
        "\n",
        "        return ids, text_input, attention_masks, labels_output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaConfig, RobertaModel\n",
        "\n",
        "\n",
        "class TextEncoderRoBERTa(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TextEncoderRoBERTa, self).__init__()\n",
        "\n",
        "        self.bert_config = RobertaConfig.from_pretrained('roberta-base',\n",
        "                                                 output_hidden_states=True,\n",
        "                                                 num_hidden_layers=10)\n",
        "\n",
        "        self.bert_model = RobertaModel.from_pretrained('roberta-base', config=self.bert_config)\n",
        "\n",
        "        self.classifier = nn.Linear(self.bert_config.hidden_size, 20)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, text_inputs, attention_mask):\n",
        "\n",
        "        outputs = self.bert_model(text_inputs, attention_mask=attention_mask)\n",
        "\n",
        "        sequence_output = outputs[0][:,0,:]  # Take the output of the [CLS] token for classification\n",
        "\n",
        "        logits = self.classifier(sequence_output)  # Pass through the classifier\n",
        "        # probs = torch.sigmoid(logits)  # Apply sigmoid to get probabilities for each label\n",
        "\n",
        "        # return probs\n",
        "        return logits"
      ],
      "metadata": {
        "id": "0jBzJX1Yebo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrGM2vXfE2b1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# import task1_dataset\n",
        "# from task1_dataset import Task1Dataset\n",
        "# from task1_collate import Collate\n",
        "# from task1_bert_model import TextEncoderBERT\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('scorer-baseline')\n",
        "# import subtask_1_2a\n",
        "\n",
        "\n",
        "def evaluate(pred, gold, CLASSES):\n",
        "\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    mlb.fit([CLASSES])\n",
        "\n",
        "    gold = mlb.transform(gold)\n",
        "    pred = mlb.transform(pred)\n",
        "\n",
        "    macro_f1 = f1_score(gold, pred, average=\"macro\", zero_division=1)\n",
        "    micro_f1 = f1_score(gold, pred, average=\"micro\", zero_division=1)\n",
        "\n",
        "    return macro_f1, micro_f1\n",
        "\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "    task1_classlist = '/path_to/task1_class_list.txt'\n",
        "\n",
        "    traindata_path = '/path_to/train.json'\n",
        "    train_dataset = Task1Dataset(traindata_path, task1_classlist)\n",
        "\n",
        "    valdata_path = '/path_to_subtask1/validation.json'\n",
        "    val_dataset = Task1Dataset(valdata_path, task1_classlist)\n",
        "\n",
        "    collate_fn = Collate(task1_classlist)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=mp.cpu_count(), collate_fn=collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=mp.cpu_count(), collate_fn=collate_fn)\n",
        "\n",
        "    model_name = 'roberta'\n",
        "    model = TextEncoderRoberta()\n",
        "    #model = torch.nn.DataParallel(model)\n",
        "    model = model.cuda()\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    optimizer = torch.optim.AdamW([\n",
        "                              {\"params\": model.bert_model.parameters(), \"lr\": 0.00002},\n",
        "                              {\"params\": model.classifier.parameters(), \"lr\": 0.0002}],lr=0.0002)\n",
        "\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[4, 8], gamma=0.8)\n",
        "\n",
        "\n",
        "\n",
        "    # used to create directory for saving checkpoints\n",
        "    cur_date_time = datetime.now()\n",
        "    date_time_str = cur_date_time.strftime('%Y-%m-%d_%H-%M')\n",
        "    run_dir = f\"task1_checkpoints/run_{date_time_str}\"\n",
        "\n",
        "    if not os.path.exists(run_dir):\n",
        "        os.makedirs(run_dir)\n",
        "\n",
        "\n",
        "    losses = []\n",
        "    num_epochs = 10\n",
        "    best_micro_f1 = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        steps = 0\n",
        "        progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "\n",
        "\n",
        "        for batch in progress_bar:\n",
        "\n",
        "            ids, text_input, attention_masks, labels_output = batch\n",
        "\n",
        "            text = text_input.cuda()\n",
        "            labels = labels_output.cuda()\n",
        "            attention_masks = attention_masks.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(text, attention_masks)\n",
        "            loss = criterion(logits, labels.float())\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "\n",
        "\n",
        "        mean_loss = torch.mean(torch.tensor(losses)).item()\n",
        "\n",
        "        print (\"=========================== Epoch: \", epoch+1,\" | Loss: \", mean_loss, \"===========================\\n\")\n",
        "\n",
        "        # if (epoch) % 2 == 0:\n",
        "\n",
        "        metrics = validate(val_dataloader, model, run_dir)\n",
        "        print(\"Validation: \", metrics, \"\\n\")\n",
        "\n",
        "        if metrics['h_f1_score = '] > best_micro_f1:\n",
        "\n",
        "            best_micro_f1 = metrics['h_f1_score = ']\n",
        "            best_epoch = epoch\n",
        "\n",
        "\n",
        "            torch.save({\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer_dict': optimizer.state_dict(),\n",
        "                'epoch': best_epoch,\n",
        "            }, run_dir +'/'+ model_name + '_' + f\"{best_micro_f1:.4f}\" + '.pth')\n",
        "\n",
        "\n",
        "            lines_to_write = [\"=========================== Epoch: \", str(epoch+1), \" | Loss: \", str(mean_loss), \"===========================\\n\",\n",
        "                              \"Validation: \", str(metrics), \"\\n\"]\n",
        "\n",
        "            with open(run_dir +'/best.txt', 'w') as f:\n",
        "                f.writelines(lines_to_write)\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # testing\n",
        "\n",
        "    devdata_path = '/path_to/dev_subtask1_en.json'\n",
        "\n",
        "    dev_dataset = Task1Dataset(devdata_path, task1_classlist)\n",
        "\n",
        "    dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False, num_workers=mp.cpu_count(), collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "    test_run_dir = f\"task1_evaluation/run_{date_time_str}\"\n",
        "\n",
        "    if not os.path.exists(test_run_dir):\n",
        "        os.makedirs(test_run_dir)\n",
        "\n",
        "\n",
        "\n",
        "    test_metrics = validate(dev_dataloader, model, test_run_dir)\n",
        "\n",
        "    print(\"Test: \", test_metrics, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    test_lines_to_write = [\"Test: \", str(test_metrics), \"\\n\"]\n",
        "\n",
        "    with open(run_dir +'/test_results.txt', 'w') as f:\n",
        "        f.writelines(test_lines_to_write)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def validate(val_dataloader, model, run_dir):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    ids_list = []\n",
        "\n",
        "    pred_list = []\n",
        "    gold_list = []\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    valdata_classlist = '/path_to/task1_class_list.txt'\n",
        "    classes_list = read_classes(valdata_classlist)\n",
        "\n",
        "    progress_bar = tqdm(val_dataloader, desc='Validation', leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "\n",
        "        ids, text_input, attention_masks, labels_output = batch\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            text = text_input.cuda()\n",
        "            labels = labels_output.cuda()\n",
        "            attention_masks = attention_masks.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_probs = model(text, attention_masks)\n",
        "            pred_classes = (pred_probs > 0.5).long()\n",
        "\n",
        "\n",
        "        predictions.extend(pred_classes.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        ids_list.extend(ids)\n",
        "\n",
        "    # Convert binary arrays to label sets for evaluation\n",
        "    preds = [set(classes_list[i] for i, label in enumerate(sample) if label == 1) for sample in predictions]\n",
        "    gts = [set(classes_list[i] for i, label in enumerate(sample) if label == 1) for sample in true_labels]\n",
        "\n",
        "\n",
        "    for id, pred in zip(ids_list, preds):    # loop over every element of the batch\n",
        "        pred_list.append({'id': id, 'labels': list(pred)})\n",
        "\n",
        "    preds_json = run_dir +'/preds.json'\n",
        "    with open(preds_json, 'w') as f:\n",
        "        json.dump(pred_list, f)\n",
        "\n",
        "\n",
        "\n",
        "    for id, gt in zip(ids_list, gts):    # loop over every element of the batch\n",
        "        gold_list.append({'id': id, 'labels': list(gt)})\n",
        "\n",
        "    gt_json = run_dir +'/gold.json'\n",
        "    with open(gt_json, 'w') as f:\n",
        "        json.dump(gold_list, f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    macro_f1, micro_f1 = evaluate(preds, gts, classes_list)\n",
        "\n",
        "    precision, recall, f1 = evaluate_h(preds_json, gt_json)\n",
        "\n",
        "    metrics['macroF1 = '] = macro_f1\n",
        "    metrics['microF1 = '] = micro_f1\n",
        "\n",
        "    metrics['h_precision = '] = precision\n",
        "    metrics['h_recall = '] = recall\n",
        "    metrics['h_f1_score = '] = f1\n",
        "\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(pred, gold, CLASSES):\n",
        "\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    mlb.fit([CLASSES])\n",
        "\n",
        "    gold = mlb.transform(gold)\n",
        "    pred = mlb.transform(pred)\n",
        "\n",
        "    macro_f1 = f1_score(gold, pred, average=\"macro\", zero_division=1)\n",
        "    micro_f1 = f1_score(gold, pred, average=\"micro\", zero_division=1)\n",
        "\n",
        "    return macro_f1, micro_f1"
      ],
      "metadata": {
        "id": "Plf_p5BDgfHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuFZcN9vVzuL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.multiprocessing as mp\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# import task1_dataset\n",
        "# from task1_dataset import Task1Dataset\n",
        "# from task1_collate import Collate\n",
        "# from task1_bert_model import TextEncoderBERT\n",
        "\n",
        "# import sys\n",
        "# sys.path.append('scorer-baseline')\n",
        "# import subtask_1_2a\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def validate(val_dataloader, model):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    ids_list = []\n",
        "\n",
        "    pred_list = []\n",
        "    gold_list = []\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    valdata_classlist = '/path_to/task1_class_list.txt'\n",
        "    classes_list = read_classes(valdata_classlist)\n",
        "\n",
        "    progress_bar = tqdm(val_dataloader, desc='Validation', leave=False)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "\n",
        "        ids, text_input, attention_masks, labels_output = batch\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            text = text_input.cuda()\n",
        "            labels = labels_output.cuda()\n",
        "            attention_masks = attention_masks.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_probs = model(text, attention_masks)\n",
        "            pred_classes = (pred_probs > 0.5).long()\n",
        "\n",
        "\n",
        "        predictions.extend(pred_classes.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        ids_list.extend(ids)\n",
        "\n",
        "    # Convert binary arrays to label sets for evaluation\n",
        "    preds = [set(classes_list[i] for i, label in enumerate(sample) if label == 1) for sample in predictions]\n",
        "    gts = [set(classes_list[i] for i, label in enumerate(sample) if label == 1) for sample in true_labels]\n",
        "\n",
        "\n",
        "    for id, pred in zip(ids_list, preds):    # loop over every element of the batch\n",
        "        pred_list.append({'id': id, 'labels': list(pred)})\n",
        "\n",
        "    preds_json = '/preds.json'\n",
        "    with open(preds_json, 'w') as f:\n",
        "        json.dump(pred_list, f)\n",
        "\n",
        "\n",
        "\n",
        "    for id, gt in zip(ids_list, gts):    # loop over every element of the batch\n",
        "        gold_list.append({'id': id, 'labels': list(gt)})\n",
        "\n",
        "    gt_json = '/gold.json'\n",
        "    with open(gt_json, 'w') as f:\n",
        "        json.dump(gold_list, f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    macro_f1, micro_f1 = evaluate(preds, gts, classes_list)\n",
        "\n",
        "    precision, recall, f1 = evaluate_h(preds_json, gt_json)\n",
        "\n",
        "    metrics['macroF1 = '] = macro_f1\n",
        "    metrics['microF1 = '] = micro_f1\n",
        "\n",
        "    metrics['h_precision = '] = precision\n",
        "    metrics['h_recall = '] = recall\n",
        "    metrics['h_f1_score = '] = f1\n",
        "\n",
        "\n",
        "    return metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu00FlmoWJWw"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "\n",
        "devdata_path = '/path_to/test_subtask1_ar.json'\n",
        "task1_classlist = '/path_to/task1_class_list.txt'\n",
        "\n",
        "dev_dataset = Task1Dataset(devdata_path, task1_classlist)\n",
        "\n",
        "collate_fn = Collate(task1_classlist)\n",
        "\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=8, shuffle=False, num_workers=mp.cpu_count(), collate_fn=collate_fn)\n",
        "\n",
        "model_name = 'roberta'\n",
        "model = TextEncoderRoBERTa()\n",
        "\n",
        "model_ckpt_path = '/path_to/Multilingual/roberta_0.5978.pth'\n",
        "\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint = torch.load(model_ckpt_path, map_location='cpu')\n",
        "\n",
        "# Check if the model was saved with DataParallel or DistributedDataParallel\n",
        "if list(checkpoint.keys())[0].startswith('module.'):\n",
        "    # Create a new state dict without the 'module.' prefix\n",
        "    new_state_dict = {k.replace('module.', ''): v for k, v in checkpoint.items()}\n",
        "else:\n",
        "    new_state_dict = checkpoint\n",
        "\n",
        "\n",
        "# Load the modified state dict into your model\n",
        "model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "model = torch.nn.DataParallel(model)\n",
        "model = model.cuda()\n",
        "\n",
        "test_metrics = validate(dev_dataloader, model)\n",
        "\n",
        "print(test_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BbQ0GWwefEe3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}